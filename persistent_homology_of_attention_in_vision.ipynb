{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8c4c75a-8820-4002-aaf9-b9b168ea30c2",
   "metadata": {},
   "source": [
    "# Persistent Homology of Attention in Vision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03aaf01-5540-424b-a935-9f55b2885173",
   "metadata": {},
   "source": [
    "![simplicial_complex_image.png](simplicial_complex_image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05588585-549c-484f-b943-fcaf75625332",
   "metadata": {},
   "source": [
    "This code is for analyzing and comparing the attention mechanisms of two images processed by Vision Transformer (ViT) models. The analysis is done using topological data analysis (TDA), a mathematical technique that studies the shape of data. In this case, TDA is used to examine the structure of attention maps generated by the vision models.\n",
    "\n",
    "Persistent homology is a technique in topological data analysis (TDA) that quantifies the topological features of data across different scales. In this code, persistent homology is used to analyze the attention mechanisms of Vision Transformer (ViT) models to gain insights into how the models process and attend to different parts of the input images.\n",
    "\n",
    "The code constructs simplicial complexes using the Rips complex method, which is built upon the pairwise distances between the attention matrix's rows (in this case, the rows represent the attention values of each patch in an image). The Rips complex is a way to represent the structure of the data, as it captures the relationships between patches in the image based on their attention values. Each simplex in the complex represents a group of patches that are considered \"close\" to each other in terms of their attention values.\n",
    "\n",
    "Persistence diagrams are then generated from the simplicial complexes, representing the topological features (e.g., connected components) of the data across different scales. These diagrams consist of points, where each point's coordinates (birth, death) represent the scale at which a topological feature appears and disappears, respectively. In this context, the birth and death of a feature correspond to the threshold distance at which patches become connected or disconnected based on their attention values.\n",
    "\n",
    "Persistent homology is useful for analyzing vision transformers because it provides a way to study the structure and organization of the attention mechanisms in these models. By examining the topological features of the attention maps, we can gain insights into how the model attends to different parts of the input images and how the attention is organized across the image patches.\n",
    "\n",
    "Comparing the persistence diagrams of two different images processed by the same ViT model can reveal similarities and differences in the attention patterns generated by the model for the respective images. The bottleneck distance, which is computed in this code, is a metric that quantifies the similarity between two persistence diagrams. A smaller bottleneck distance indicates that the topological features of the attention maps for the two images are more similar, suggesting that the model processes the images in a similar manner.\n",
    "\n",
    "In summary, using persistent homology to analyze the attention mechanisms of vision transformers can help us:\n",
    "\n",
    "1. Understand the structure and organization of the attention mechanisms within the model and how they change across different layers and heads.\n",
    "2. Identify similarities and differences in the attention patterns generated by the model for different input images.\n",
    "3. Evaluate the robustness and interpretability of the model's attention mechanisms by studying the topological features and their stability across different scales.\n",
    "\n",
    "Overall, the application of persistent homology to study vision transformers can contribute to a better understanding of these models, potentially leading to improvements in their performance, robustness, and interpretability.\n",
    "\n",
    "---\n",
    "\n",
    "Please run the cells below, and upload two photos to compare. Once the two photos are uploaded, please select a layer and head to study. Then select a scale. This will give a simplicial complex (technically its $1$-skeleton) that represents the topology of the attention scores at that scale. The persistence diagrams are also computed, along with their bottleneck distance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "066390bf-84ba-4f87-bf25-53b6a6c363d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53f2d3ae-cb0f-4960-9b83-208a0db51ba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install numpy torch transformers gudhi matplotlib networkx scipy plotly ipywidgets -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0636c22-ebaf-4ff9-8a4c-754c7ebdbeec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amelieschreiber/opt/anaconda3/lib/python3.9/site-packages/transformers/models/vit/feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at google/vit-base-patch16-224 were not used when initializing ViTModel: ['classifier.weight', 'classifier.bias']\n",
      "- This IS expected if you are initializing ViTModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ViTModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "385bbc3e99694bf1843b34975bd156a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.05, continuous_update=False, description='Threshold:', max=0.5, stepâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9138ba116dc9462d953b454ffb5c5547",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(FileUpload(value={}, description='Image 1:'), FileUpload(value={}, description='Image 2:')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel, ViTFeatureExtractor\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import gudhi as gd\n",
    "import networkx as nx\n",
    "from scipy.spatial.distance import jensenshannon\n",
    "import plotly.graph_objs as go\n",
    "from ipywidgets import interact, FloatSlider, IntSlider, Dropdown, VBox, Label, FileUpload\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "def plot_persistence_diagram(persistence, title):\n",
    "    gd.plot_persistence_diagram(persistence)\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "    \n",
    "def compute_bottleneck_distance(persistence1, persistence2):\n",
    "    persistence1_array = np.array([(birth, death) for dim, (birth, death) in persistence1 if dim == 0])\n",
    "    persistence2_array = np.array([(birth, death) for dim, (birth, death) in persistence2 if dim == 0])\n",
    "    return gd.bottleneck_distance(persistence1_array, persistence2_array)\n",
    "\n",
    "def load_model(model_name):\n",
    "    feature_extractor = ViTFeatureExtractor.from_pretrained(model_name)\n",
    "    model = AutoModel.from_pretrained(model_name)\n",
    "    return feature_extractor, model\n",
    "\n",
    "# Replace language models with vision transformers\n",
    "model_dropdown = Dropdown(\n",
    "    options=[\n",
    "        ('ViT Base Patch16 224', 'google/vit-base-patch16-224'),\n",
    "        ('ViT Large Patch16 224', 'google/vit-large-patch16-224')\n",
    "    ],\n",
    "   \n",
    "        value='google/vit-base-patch16-224',\n",
    "        description='Model:'\n",
    "    )\n",
    "\n",
    "feature_extractor, model = load_model(model_dropdown.value)\n",
    "\n",
    "# Replace text inputs with file upload widgets for images\n",
    "image_input1 = FileUpload(description='Image 1:')\n",
    "image_input2 = FileUpload(description='Image 2:')\n",
    "\n",
    "layer_slider = IntSlider(value=1, min=0, max=model.config.num_hidden_layers - 1, description='Layer:', continuous_update=False)\n",
    "head_slider = IntSlider(value=2, min=0, max=model.config.num_attention_heads - 1, description='Head:', continuous_update=False)\n",
    "\n",
    "threshold_slider = FloatSlider(value=0.05, min=0.00, max=0.5, step=0.001, description='Threshold:', continuous_update=False)\n",
    "\n",
    "\n",
    "def compute_persistence(attention_matrix):\n",
    "    softmax_attention = np.exp(attention_matrix) / np.sum(np.exp(attention_matrix), axis=-1)[:, np.newaxis]\n",
    "    distance_matrix = np.array([[np.sqrt(jensenshannon(softmax_attention[i], softmax_attention[j])) for j in range(softmax_attention.shape[0])] for i in range(softmax_attention.shape[0])])\n",
    "    \n",
    "    rips_complex = gd.RipsComplex(distance_matrix=distance_matrix, max_edge_length=np.inf)\n",
    "    simplex_tree = rips_complex.create_simplex_tree(max_dimension=2)\n",
    "    persistence = simplex_tree.persistence(min_persistence=0.01)\n",
    "    return persistence, simplex_tree, distance_matrix\n",
    "\n",
    "def plot_simplicial_complex_3d(simplex_tree, distance_matrix, title, threshold, tokens):\n",
    "    g = nx.Graph()\n",
    "    for (simplex, _) in simplex_tree.get_filtration():\n",
    "        if len(simplex) == 2:\n",
    "            if distance_matrix[simplex[0]][simplex[1]] <= threshold:\n",
    "                g.add_edge(simplex[0], simplex[1])\n",
    "\n",
    "    labels = {node: tokens[node] for node in g.nodes()}\n",
    "    \n",
    "    pos = nx.spring_layout(g, dim=3, seed=42)\n",
    "    \n",
    "    Xn = [pos[k][0] for k in g.nodes()]\n",
    "    Yn = [pos[k][1] for k in g.nodes()]\n",
    "    Zn = [pos[k][2] for k in g.nodes()]\n",
    "    \n",
    "    Xe = []\n",
    "    Ye = []\n",
    "    Ze = []\n",
    "    for e in g.edges():\n",
    "        Xe += [pos[e[0]][0], pos[e[1]][0], None]\n",
    "        Ye += [pos[e[0]][1], pos[e[1]][1], None]\n",
    "        Ze += [pos[e[0]][2], pos[e[1]][2], None]\n",
    "    \n",
    "    trace_edges = go.Scatter3d(x=Xe, y=Ye, z=Ze, mode='lines', line=dict(color='gray', width=1))\n",
    "    \n",
    "    trace_nodes = go.Scatter3d(x=Xn, y=Yn, z=Zn, mode='markers+text', text=list(labels.values()), marker=dict(symbol='circle', size=10, color='lightblue'), textposition=\"top center\")\n",
    "    \n",
    "    layout = go.Layout(title=title, scene=dict(xaxis_title='X', yaxis_title='Y', zaxis_title='Z'), showlegend=False)\n",
    "    \n",
    "    fig = go.Figure(data=[trace_edges, trace_nodes], layout=layout)\n",
    "    fig.show()\n",
    "    \n",
    "\n",
    "def get_attention_matrix(image, model, feature_extractor, layer, head):\n",
    "    # Convert the image to RGB format if it is not already\n",
    "    if image.mode != \"RGB\":\n",
    "        image = image.convert(\"RGB\")\n",
    "\n",
    "    inputs = feature_extractor(images=image, return_tensors=\"pt\")\n",
    "    outputs = model(**inputs, output_attentions=True)\n",
    "    attention = outputs.attentions[layer][0, head].detach().cpu().numpy()\n",
    "\n",
    "    return attention\n",
    "\n",
    "\n",
    "def update_plot(threshold, layer, head, model_name):\n",
    "    if len(image_input1.data) == 0 or len(image_input2.data) == 0:\n",
    "        print(\"Please upload both images before proceeding.\")\n",
    "        return\n",
    "\n",
    "    tokenizer, model = load_model(model_name)\n",
    "    layer_slider.max = model.config.num_hidden_layers - 1\n",
    "    head_slider.max = model.config.num_attention_heads - 1\n",
    "\n",
    "    # Load images from the uploaded files\n",
    "    image1 = Image.open(BytesIO(image_input1.data[-1]))\n",
    "    image2 = Image.open(BytesIO(image_input2.data[-1]))\n",
    "\n",
    "    attention_matrix1 = get_attention_matrix(image1, model, tokenizer, layer, head)\n",
    "    attention_matrix2 = get_attention_matrix(image2, model, tokenizer, layer, head)\n",
    "\n",
    "    persistence1, simplex_tree1, distance_matrix1 = compute_persistence(attention_matrix1)\n",
    "    persistence2, simplex_tree2, distance_matrix2 = compute_persistence(attention_matrix2)\n",
    "\n",
    "    plot_simplicial_complex_3d(simplex_tree1, distance_matrix1, \"Simplicial Complex for Image 1\", threshold, range(attention_matrix1.shape[0]))\n",
    "    plot_simplicial_complex_3d(simplex_tree2, distance_matrix2, \"Simplicial Complex for Image 2\", threshold, range(attention_matrix2.shape[0]))\n",
    "    \n",
    "    plot_persistence_diagram(persistence1, \"Persistence Diagram for Image 1\")\n",
    "    plot_persistence_diagram(persistence2, \"Persistence Diagram for Image 2\")\n",
    "\n",
    "    # Compute and display bottleneck distance between persistence diagrams\n",
    "    bottleneck_distance = compute_bottleneck_distance(persistence1, persistence2)\n",
    "    print(\"Bottleneck distance between Image 1 and Image 2:\", bottleneck_distance)\n",
    "\n",
    "interact(update_plot, threshold=threshold_slider, layer=layer_slider, head=head_slider, model_name=model_dropdown)\n",
    "\n",
    "# Display the image upload widgets\n",
    "VBox([image_input1, image_input2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cccf70c0-7305-4917-af2b-4948d1ae7a0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('anaconda3': virtualenv)",
   "language": "python",
   "name": "python385jvsc74a57bd0474c67ce7e36ad5731492349411c4ce02ca5c170a680b2d1efe1eb0325e35fe7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
