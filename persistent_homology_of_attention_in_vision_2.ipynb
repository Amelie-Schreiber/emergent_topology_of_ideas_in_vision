{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing Contextual Mappings of Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -atplotlib (/Users/amelieschreiber/anaconda3/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -atplotlib (/Users/amelieschreiber/anaconda3/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -atplotlib (/Users/amelieschreiber/anaconda3/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -atplotlib (/Users/amelieschreiber/anaconda3/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -atplotlib (/Users/amelieschreiber/anaconda3/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -atplotlib (/Users/amelieschreiber/anaconda3/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision transformers timm ipywidgets -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/vit-base-patch16-224 were not used when initializing ViTModel: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing ViTModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ViTModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f553db6c32004730aefb6fef681a4140",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntText(value=1, description='Patch index A:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1af5cb6240464f9b9700bad431fba9d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntText(value=1, description='Patch index B:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f02c1631b46f43d09339cd201c4e40ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Compute cosine similarity', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity for patch indices 178 and 76: 0.6201204061508179\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import ViTModel, ViTConfig\n",
    "from torchvision import transforms\n",
    "from PIL import Image, ImageDraw\n",
    "import ipywidgets as widgets\n",
    "import numpy as np\n",
    "import math \n",
    "\n",
    "def display_image_with_patches(image, patch_size, title):\n",
    "    image_width, image_height = image.size\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    patch_id = 0\n",
    "\n",
    "    for y in range(0, image_height, patch_size):\n",
    "        for x in range(0, image_width, patch_size):\n",
    "            draw.rectangle([x, y, x + patch_size, y + patch_size], outline=\"red\", width=2)\n",
    "            draw.text((x + 5, y + 5), str(patch_id), fill=\"red\")\n",
    "            patch_id += 1\n",
    "\n",
    "    image.show(title=title)\n",
    "\n",
    "    \n",
    "# Load a pre-trained Vision Transformer model\n",
    "config = ViTConfig.from_pretrained(\"google/vit-base-patch16-224\")\n",
    "model = ViTModel.from_pretrained(\"google/vit-base-patch16-224\")\n",
    "\n",
    "# Prepare the image inputs\n",
    "image_path_A = \"heart.png\"\n",
    "image_path_B = \"heart2.png\"\n",
    "image_A = Image.open(image_path_A).convert(\"RGB\")\n",
    "image_B = Image.open(image_path_B).convert(\"RGB\")\n",
    "\n",
    "num_patches_sqrt = int(math.sqrt(config.num_hidden_layers))\n",
    "patch_size = 224 // num_patches_sqrt\n",
    "\n",
    "# Display images with labeled patches\n",
    "display_image_with_patches(image_A.copy(), patch_size, title=\"Image A with Patches\")\n",
    "display_image_with_patches(image_B.copy(), patch_size, title=\"Image B with Patches\")\n",
    "\n",
    "\n",
    "# Transform the images\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "image_input_A = transform(image_A).unsqueeze(0)  # Add batch dimension\n",
    "image_input_B = transform(image_B).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "# Pass the images through the model to get features\n",
    "with torch.no_grad():\n",
    "    outputs_A = model(image_input_A)\n",
    "    outputs_B = model(image_input_B)\n",
    "    hidden_states_A = outputs_A.last_hidden_state\n",
    "    hidden_states_B = outputs_B.last_hidden_state\n",
    "\n",
    "# Function to compute cosine similarity between image patches\n",
    "def compute_cosine_similarity(patch_index_A, patch_index_B):\n",
    "    patch_vector_A = hidden_states_A[0, patch_index_A, :]\n",
    "    patch_vector_B = hidden_states_B[0, patch_index_B, :]\n",
    "    \n",
    "    cosine_similarity = torch.nn.functional.cosine_similarity(patch_vector_A.unsqueeze(0), patch_vector_B.unsqueeze(0))\n",
    "    return cosine_similarity.item()\n",
    "\n",
    "# Create input boxes for patch indices\n",
    "patch_index_box_A = widgets.IntText(\n",
    "    value=1,\n",
    "    description='Patch index A:',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "patch_index_box_B = widgets.IntText(\n",
    "    value=1,\n",
    "    description='Patch index B:',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "# Create a button to trigger the computation of cosine similarity\n",
    "button = widgets.Button(description=\"Compute cosine similarity\")\n",
    "\n",
    "def on_button_click(b):\n",
    "    cosine_similarity = compute_cosine_similarity(patch_index_box_A.value, patch_index_box_B.value)\n",
    "    print(f\"Cosine similarity for patch indices {patch_index_box_A.value} and {patch_index_box_B.value}: {cosine_similarity}\")\n",
    "\n",
    "button.on_click(on_button_click)\n",
    "\n",
    "# Display the UI\n",
    "display(patch_index_box_A)\n",
    "display(patch_index_box_B)\n",
    "display(button)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
